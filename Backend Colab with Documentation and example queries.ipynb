{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbyuRdPmH59I"
      },
      "source": [
        "### Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "i5WVVZHkG7o3",
        "outputId": "f797edab-e8f9-48db-ad0f-99d919e7d9a0"
      },
      "outputs": [],
      "source": [
        "%pip install llama_index\n",
        "%pip install llama-index-vector-stores-chroma\n",
        "%pip install llama_index-embeddings-gemini\n",
        "%pip install llama-index-llms-gemini\n",
        "%pip install PyPDF2\n",
        "%pip install chromadb\n",
        "%pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Based PDF Chatbot Architecture Overview\n",
        "\n",
        "This notebook outlines the architecture of an AI-based PDF chatbot that combines multiple components to create an interactive QA bot capable of retrieving information from documents and generating responses using Google’s Gemini generative AI model. Below is a detailed breakdown of the system structure and functionality:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Document Handling\n",
        "\n",
        "### PDF or TXT Input:\n",
        "- The bot can load text data from either **.txt** or **.pdf** files.\n",
        "- PDF loading is handled using the **PyPDF2** library, which extracts raw text from PDFs and converts it into a single document object.\n",
        "\n",
        "### Text Splitting and Chunking:\n",
        "- The document is split into manageable chunks using the **SentenceSplitter**.\n",
        "- Each chunk has a defined size (`chunk_size=512`) and overlap (`chunk_overlap=20`) to ensure smoother transitions between sentences across document splits.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Vector Store & Embeddings\n",
        "\n",
        "### Vector Store with Chroma:\n",
        "- Document chunks are stored in a vector database, **ChromaVectorStore**, provided by `chromadb`.\n",
        "- **Chroma** handles vector-based searches by comparing the user query embeddings to the document chunk embeddings, allowing the chatbot to retrieve relevant sections from the document.\n",
        "\n",
        "### Embeddings:\n",
        "- The bot uses **GeminiEmbedding** for embedding the document text into vectors that enable efficient vector-based retrieval.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Generative Model\n",
        "\n",
        "### LLM (Large Language Model):\n",
        "- The core LLM is Google’s **Gemini** model, which powers the generative response capability of the chatbot.\n",
        "- The model used is **gemini-pro**, an advanced version of Google's Generative AI.\n",
        "\n",
        "### LLM Settings:\n",
        "- Configurations such as **API keys**, **model name**, and output limits (`num_output=512`) are defined to control the generation of responses from the model.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "### Index Creation:\n",
        "- After the document is split and embedded, the vectorized chunks are indexed using **VectorStoreIndex**.\n",
        "- This index is used to retrieve the most relevant document segments based on a user’s query.\n",
        "\n",
        "### Retrieval:\n",
        "- When a query is sent to the bot, the vector store is queried to retrieve relevant document chunks that match the query's semantics.\n",
        "\n",
        "### Response Generation:\n",
        "- The **Gemini model** generates responses by considering both the retrieved document segments and the ongoing conversation context.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Chat Engine\n",
        "\n",
        "### Chat Engine:\n",
        "- The `_create_chat_engine()` function sets up the chat engine using `as_chat_engine`, where retrieved document segments are fed into the generative model to create a final response.\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow of Interaction:\n",
        "\n",
        "1. **Document Upload**: The user uploads a **.txt** or **.pdf** file.\n",
        "2. **Vector Index Creation**: \n",
        "   - The document is chunked, embedded using **GeminiEmbedding**, and indexed in **Chroma’s vector store**.\n",
        "3. **Query Input**: \n",
        "   - The user submits a query.\n",
        "4. **Document Retrieval**: \n",
        "   - The vector index retrieves relevant document sections based on the semantic similarity of the query and the document chunks.\n",
        "5. **Response Generation**: \n",
        "   - The **generative model** creates a final answer by combining the retrieved segments and any prior conversation context.\n",
        "6. **User Interaction**: \n",
        "   - The response is presented to the user, along with relevant document sections (if any were retrieved).\n",
        "\n",
        "---\n",
        "\n",
        "## Generative Responses\n",
        "\n",
        "The chatbot generates answers based on a combination of the following:\n",
        "\n",
        "### Document-based Retrieval:\n",
        "- The bot primarily refers to the uploaded document to generate accurate and document-specific answers.\n",
        "\n",
        "\n",
        "### Generative Completion:\n",
        "- If the document and memory do not provide sufficient information, the bot can generate an answer based on its general knowledge or indicate that the information is unavailable.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Challenges faced, and solutions\n",
        "\n",
        "## Key Decisions and Design\n",
        "\n",
        "### 1. Choosing Google Gemini API for LLM and Embeddings\n",
        "- **Decision**: Use `Google Generative AI` and `Gemini` models for both the language model (LLM) and embeddings.\n",
        "- **Reason**: `Gemini` offers state-of-the-art language understanding and embeddings, ensuring performance and consistency.\n",
        "- **Challenge**: API key integration and request management.\n",
        "- **Solution**: Set up credentials using environment variables and configure API keys during app initialization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Using ChromaDB for Vector Storage\n",
        "- **Decision**: Utilize `ChromaDB` for vector storage and retrieval during QA.\n",
        "- **Reason**: Efficiently stores document embeddings and scales easily for retrieval tasks.\n",
        "- **Challenge**: Managing the vector store without creating conflicts.\n",
        "- **Solution**: Check if a collection exists, delete it before creating a new one, ensuring up-to-date knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Supporting PDF and TXT Files\n",
        "- **Decision**: Allow uploads of both `.pdf` and `.txt` files.\n",
        "- **Reason**: Common formats for users and increases accessibility.\n",
        "- **Challenge**: Handling PDF extraction from complex layouts.\n",
        "- **Solution**: Use `PyPDF2` for extracting PDF text and wrap it in `Document` objects for uniformity between formats.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Chunking Documents for Efficient Embedding\n",
        "- **Decision**: Split documents into smaller chunks using `SentenceSplitter` with a 512-token chunk size and 20-token overlap.\n",
        "- **Reason**: Avoid overloading the LLM’s context window, improving performance and accuracy.\n",
        "- **Challenge**: Retaining document context while splitting.\n",
        "- **Solution**: Include a 20-token overlap between chunks to maintain continuity in responses.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Creating the Chat Engine\n",
        "- **Decision**: Implement a conversational context using `ChatMemoryBuffer` and an LLM-powered chat engine.\n",
        "- **Reason**: Handle follow-up questions with context for better user experience.\n",
        "- **Challenge**: Managing memory size to handle long conversations without exceeding token limits.\n",
        "- **Solution**: Set a large token limit of 150,000 tokens to balance context retention with performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Challenges and Solutions\n",
        "\n",
        "### 1. File Handling for Different Formats\n",
        "- **Challenge**: Efficient handling of both TXT and PDF file formats.\n",
        "- **Solution**: Use `PyPDF2` for PDF extraction and `SimpleDirectoryReader` for TXT files, ensuring robustness in loading documents.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Ensuring Embedding Consistency and Efficiency\n",
        "- **Challenge**: Embedding large documents efficiently without overloading the system.\n",
        "- **Solution**: Split documents into manageable chunks and embed them using `Google GeminiEmbedding`, ensuring the vector store remains scalable.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Managing the Knowledge Base\n",
        "- **Challenge**: Efficiently updating the ChromaDB vector store for new document uploads.\n",
        "- **Solution**: Delete outdated collections and recreate them during every new upload, ensuring fresh embeddings and avoiding bottlenecks.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Graceful Error Handling\n",
        "- **Challenge**: Handling errors during file loading or API interaction without crashing the app.\n",
        "- **Solution**: Implement `try-except` blocks with meaningful error messages to inform users about issues like failed PDF extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. User-Friendly Interaction with the LLM\n",
        "- **Challenge**: Ensuring the bot provides accurate answers based only on the uploaded document.\n",
        "- **Solution**: Use system prompts to guide the bot to answer within the document’s scope and avoid hallucinations.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **QAChatbot Class**: Centralized logic for document processing, knowledge base creation, and user interaction.\n",
        "2. **Embedding and Indexing**: Managed by the `_create_kb` method, responsible for embedding and storing documents in ChromaDB.\n",
        "3. **Chat Memory and Chat Engine**: Uses `ChatMemoryBuffer` to retain context during conversations for smooth follow-up queries.\n",
        "4. **Error Handling and Debugging**: Includes debugging print statements to track app flow and catch issues during development.\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8seGJhmIcWC"
      },
      "source": [
        "### Library Imports\n",
        "\n",
        "In this section, we import the necessary libraries for our project. Each library serves a specific purpose in our workflow.\n",
        "\n",
        "1. **`from llama_index.embeddings.gemini import GeminiEmbedding`**\n",
        "   - **Purpose:** Provides embeddings functionality using the Gemini model. Embeddings are used to convert text into vector representations for processing.\n",
        "\n",
        "2. **`from llama_index.vector_stores.chroma import ChromaVectorStore`**\n",
        "   - **Purpose:** Interfaces with Chroma, a vector database for storing and retrieving vector embeddings. Chroma is used to manage and query large sets of embeddings efficiently.\n",
        "\n",
        "3. **`from llama_index.core import SimpleDirectoryReader, VectorStoreIndex`**\n",
        "   - **Purpose:**\n",
        "     - `SimpleDirectoryReader`: Reads documents from a directory. It's useful for loading data from files.\n",
        "     - `VectorStoreIndex`: Creates an index from documents using vector embeddings. This index is used to perform similarity searches.\n",
        "\n",
        "4. **`from llama_index.core.memory import ChatMemoryBuffer`**\n",
        "   - **Purpose:** Manages memory for chat interactions, allowing the model to remember previous interactions within a session.\n",
        "\n",
        "5. **`from llama_index.core.storage.storage_context import StorageContext`**\n",
        "   - **Purpose:** Manages storage context for the vector store, handling the creation and maintenance of vector databases.\n",
        "\n",
        "6. **`from llama_index.core import Settings`**\n",
        "   - **Purpose:** Provides configuration settings for the model, including parameters for embeddings, chunking, and output.\n",
        "\n",
        "7. **`from llama_index.llms.gemini import Gemini`**\n",
        "   - **Purpose:** Interfaces with the Gemini language model for generating responses based on the provided context and queries.\n",
        "\n",
        "8. **`import PyPDF2`**\n",
        "   - **Purpose:** A library for reading and extracting text from PDF files. It is used to handle PDF document content.\n",
        "\n",
        "9. **`import chromadb`**\n",
        "   - **Purpose:** A client library for interacting with the Chroma vector database. It is used for managing collections and querying vectors.\n",
        "\n",
        "10. **`import google.generativeai as genai`**\n",
        "    - **Purpose:** Interfaces with Google's Generative AI API for generating text and other AI-driven functionalities.\n",
        "\n",
        "11. **`import warnings`**\n",
        "    - **Purpose:** Provides a way to issue and control warnings. It's used to suppress or display warnings during execution.\n",
        "\n",
        "12. **`import os`**\n",
        "    - **Purpose:** Provides a way to interact with the operating system, such as setting environment variables and managing file paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DmNVNdBQdiov"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "import PyPDF2\n",
        "import chromadb\n",
        "import google.generativeai as genai\n",
        "import warnings\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spaBY1cC85mz"
      },
      "source": [
        "### **Model Architecture**\n",
        "\n",
        "The system integrates various components to perform question-answering using a hybrid Retrieval-Augmented Generation (RAG) approach. The core components of the architecture are:\n",
        "\n",
        "1. **Document Ingestion and Preprocessing**:\n",
        "   - The documents are loaded from local files (e.g., PDFs) using the `SimpleDirectoryReader`.\n",
        "   - Preprocessing includes tokenizing the text and preparing it for embedding by chunking it into manageable parts.\n",
        "\n",
        "2. **Embeddings Generation**:\n",
        "   - A pre-trained embedding model like **GeminiEmbedding** is used to convert the text into high-dimensional vector representations. These embeddings capture semantic meaning, allowing for similarity-based retrieval later.\n",
        "   - Embeddings are created for both documents and queries to facilitate comparison.\n",
        "\n",
        "3. **Vector Storage**:\n",
        "   - The system stores the generated embeddings in a vector database such as **ChromaVectorStore**. This allows for efficient querying based on vector similarity. Each document chunk’s embedding is indexed to enable retrieval during question-answering.\n",
        "\n",
        "4. **Question-Answering Pipeline**:\n",
        "   - When a user submits a question, it is first converted into an embedding using the same **GeminiEmbedding** model.\n",
        "   - The vector store is queried to find the most similar document chunks based on the cosine similarity between the question's embedding and the document embeddings.\n",
        "   \n",
        "5. **Generative Response**:\n",
        "   - The retrieved document chunks are passed to a generative language model, such as **Gemini** or **Google Generative AI**. These models generate a coherent and contextually appropriate response by synthesizing information from the retrieved documents.\n",
        "   - The final response is generated based on a combination of the retrieved content and the model’s generative capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y13Un0_qd442"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up your Google API credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/your Google API credentials.json\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up Google Generative AI API key\n",
        "genai.configure(api_key='Google api key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUGS3lM8Jtma"
      },
      "source": [
        "### Configuration of Settings for Language Model and Embeddings\n",
        "\n",
        "In this section, we configure various settings for the language model (LLM) and the embedding model.\n",
        "\n",
        "1. **`from llama_index.core.node_parser import SentenceSplitter`**\n",
        "   - **Purpose:** Imports the `SentenceSplitter` class used to split text into manageable chunks for processing.\n",
        "\n",
        "2. **`Settings.llm = Gemini(models='gemini-pro', api_key='AIzaSyDFqHznL-EB9_CHIGFrwVfwmUBCmk4nRzc')`**\n",
        "   - **Purpose:** Configures the language model to use Gemini, specifically the 'gemini-pro' model. The `api_key` is required for accessing the Gemini API.\n",
        "   - **Why:** Setting up the LLM allows the system to generate responses based on the provided context and queries.\n",
        "\n",
        "3. **`Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\", api_key='AIzaSyDFqHznL-EB9_CHIGFrwVfwmUBCmk4nRzc')`**\n",
        "   - **Purpose:** Configures the embedding model to use `GeminiEmbedding`, specifying the model name and `api_key` for creating vector embeddings.\n",
        "   - **Why:** Embeddings are used to convert text into numerical vectors, which are necessary for indexing and similarity search.\n",
        "\n",
        "4. **`Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)`**\n",
        "   - **Purpose:** Sets the `SentenceSplitter` to split text into chunks of 512 tokens, with a 20-token overlap between chunks.\n",
        "   - **Why:** This ensures that large texts are divided into smaller parts, making it easier to process and analyze, while maintaining context between chunks.\n",
        "\n",
        "5. **`Settings.num_output = 512`**\n",
        "   - **Purpose:** Specifies the number of tokens the model should generate in its response.\n",
        "   - **Why:** Limits the response length to 512 tokens, balancing detail and conciseness.\n",
        "\n",
        "6. **`Settings.context_window = 3900`**\n",
        "   - **Purpose:** Defines the context window size, which is the number of tokens the model can consider when generating a response.\n",
        "   - **Why:** A larger context window allows the model to take more context into account, potentially improving the relevance and accuracy of the responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JLQ-s3nOdibv"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Configuring the settings for the LLM and embedding model\n",
        "Settings.llm = Gemini(models='gemini-pro', api_key='Google api key')\n",
        "# Sets the language model to Gemini with a specific API key for generating responses.\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\", api_key='Google api key')\n",
        "# Configures the embedding model to use GeminiEmbedding with a specific model name and API key for creating vector embeddings.\n",
        "\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "# Sets up the sentence splitter to process text into chunks of size 512 tokens with an overlap of 20 tokens between chunks.\n",
        "\n",
        "Settings.num_output = 512\n",
        "# Specifies the number of tokens to be generated in the model’s response.\n",
        "\n",
        "Settings.context_window = 3900\n",
        "# Sets the context window size to 3900 tokens, which determines how much text the model considers when generating a response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0MaMJRS8vgP"
      },
      "source": [
        "### **Retrieval Approach**\n",
        "\n",
        "The system employs a vector-based retrieval approach. Here's how the retrieval mechanism works:\n",
        "\n",
        "1. **Document Chunking**:\n",
        "   - Large documents are divided into smaller chunks to ensure that each part is manageable for embedding and retrieval.\n",
        "   - Each chunk is transformed into an embedding, which is a vector representation of the chunk’s semantic content.\n",
        "\n",
        "2. **Similarity Search**:\n",
        "   - When a query is received, its embedding is compared to the document embeddings stored in **ChromaVectorStore**.\n",
        "   - The similarity search is performed using cosine similarity, which measures the angle between the query vector and document vectors. Smaller angles indicate higher similarity.\n",
        "   \n",
        "3. **Retrieval of Top Matches**:\n",
        "   - The vector store returns the most relevant document chunks based on similarity. These chunks serve as the context for generating an answer to the query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGEFMRCn1SB-"
      },
      "source": [
        "### QAChatbot Class Overview\n",
        "\n",
        "The `QAChatbot` class is designed to create a chatbot that can answer questions based on the content of a document. Here’s a breakdown of the main components:\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "- **`__init__(self, document_path)`**\n",
        "  - **Purpose:** Initializes the chatbot with the path to the document and sets up the ChromaDB client and language model.\n",
        "  - **Attributes:**\n",
        "    - `self._chroma_client`: ChromaDB client for vector storage.\n",
        "    - `self._llm`: Language model (LLM) used for generating responses.\n",
        "    - `self._document_path`: Path to the document.\n",
        "    - `self._index`: Placeholder for the vector store index.\n",
        "  - **Methods Called:**\n",
        "    - `_create_kb()`: Creates the knowledge base from the document.\n",
        "    - `_create_chat_engine()`: Initializes the chat engine using the knowledge base.\n",
        "\n",
        "#### Creating the Knowledge Base\n",
        "\n",
        "- **`_create_kb(self)`**\n",
        "  - **Purpose:** Loads the document, creates a vector store, and generates a knowledge base index.\n",
        "  - **Steps:**\n",
        "    - Checks the document type (TXT or PDF) and loads the content.\n",
        "    - Deletes any existing collection in ChromaDB.\n",
        "    - Creates a new collection and sets up the vector store and index.\n",
        "  - **Why:** This method prepares the data for querying by converting it into a format that can be used by the chatbot.\n",
        "\n",
        "#### Loading PDF Documents\n",
        "\n",
        "- **`_load_pdf(self, pdf_path)`**\n",
        "  - **Purpose:** Extracts text from a PDF file and wraps it into `Document` objects.\n",
        "  - **Steps:**\n",
        "    - Opens the PDF file and reads its content.\n",
        "    - Extracts text from each page and compiles it into a `Document` object.\n",
        "  - **Why:** This method enables the chatbot to handle PDF files as input and convert them into a usable format.\n",
        "\n",
        "#### Creating the Chat Engine\n",
        "\n",
        "- **`_create_chat_engine(self)`**\n",
        "  - **Purpose:** Initializes the chat engine with context mode and memory.\n",
        "  - **Steps:**\n",
        "    - Sets up a memory buffer with a token limit.\n",
        "    - Configures the chat engine with context mode, memory, and the language model.\n",
        "  - **Why:** This method enables the chatbot to interact with users and generate responses based on the knowledge base.\n",
        "\n",
        "#### Interacting with the Language Model\n",
        "\n",
        "- **`interact_with_llm(self, user_query)`**\n",
        "  - **Purpose:** Sends a query to the chat engine and returns the generated response.\n",
        "  - **Steps:**\n",
        "    - Checks if the chat engine is initialized.\n",
        "    - Sends the user query to the chat engine and retrieves the response.\n",
        "  - **Why:** This method allows users to ask questions and receive answers from the chatbot.\n",
        "\n",
        "#### Prompt Definition\n",
        "\n",
        "- **`_prompt(self)`**\n",
        "  - **Purpose:** Provides a prompt that defines the behavior of the AI assistant.\n",
        "  - **Content:** Guides the assistant to answer questions based on the provided document and states if information is not available.\n",
        "  - **Why:** This prompt ensures that the chatbot answers questions accurately and consistently based on the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u3PP7KpiQFTB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vNuPYB-gdiN_"
      },
      "outputs": [],
      "source": [
        "class QAChatbot:\n",
        "    def __init__(self, document_path):\n",
        "        self._chroma_client = chromadb.EphemeralClient()\n",
        "        self._llm = Settings.llm\n",
        "        self._document_path = document_path\n",
        "        self._index = None\n",
        "        self._create_kb()\n",
        "        self._create_chat_engine()\n",
        "\n",
        "    def _create_kb(self):\n",
        "        try:\n",
        "            # Check if the document is a PDF or TXT\n",
        "            if self._document_path.endswith('.txt'):\n",
        "                print(\"Reading TXT file\")\n",
        "                reader = SimpleDirectoryReader(input_files=[self._document_path])\n",
        "                documents = reader.load_data()\n",
        "                print(\"Documents loaded successfully from TXT file\")\n",
        "            elif self._document_path.endswith('.pdf'):\n",
        "                documents = self._load_pdf(self._document_path)\n",
        "                print(\"Documents loaded successfully from PDF file\")\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format. Please provide a .txt or .pdf file.\")\n",
        "\n",
        "            # Check if the collection already exists and delete it\n",
        "            collection_name = \"collection\"\n",
        "            existing_collections = [col.name for col in self._chroma_client.list_collections()]\n",
        "\n",
        "            if collection_name in existing_collections:\n",
        "                self._chroma_client.delete_collection(collection_name)  # Delete the whole collection\n",
        "                print(f\"Deleted existing collection: {collection_name}\")\n",
        "\n",
        "            # Create a new collection\n",
        "            chroma_collection = self._chroma_client.create_collection(collection_name)\n",
        "            print(f\"Created new collection: {collection_name}\")\n",
        "\n",
        "            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "            # Create the vector index with documents and embeddings\n",
        "            self._index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=Settings.embed_model)\n",
        "            print(\"Knowledgebase created successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error while creating knowledgebase: {e}\")\n",
        "            self._index = None\n",
        "\n",
        "\n",
        "    def _load_pdf(self, pdf_path):\n",
        "        # Extract text from PDF\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text()\n",
        "\n",
        "            # Wrap the extracted text into Document objects\n",
        "            documents = [Document(text=text)]  # Create a list of Document objects\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PDF: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _create_chat_engine(self):\n",
        "        if self._index is None:\n",
        "            print(\"Knowledgebase is not created. Cannot create chat engine.\")\n",
        "            return\n",
        "        memory = ChatMemoryBuffer.from_defaults(token_limit=150000)\n",
        "        self._chat_engine = self._index.as_chat_engine(\n",
        "            chat_mode=\"context\",\n",
        "            memory=memory,\n",
        "            system_prompt=self._prompt,\n",
        "            llm=self._llm,\n",
        "\n",
        "        )\n",
        "\n",
        "    def interact_with_llm(self, user_query):\n",
        "        try:\n",
        "            if self._chat_engine is None:\n",
        "                print(\"Chat engine is not initialized due to knowledgebase creation failure.\")\n",
        "                return \"Sorry, the chat engine is not available.\"\n",
        "\n",
        "            print(\"Debug: Sending query to chat engine\")\n",
        "            AgentChatResponse = self._chat_engine.chat(user_query)\n",
        "            print(\"Debug: Received response from chat engine\")\n",
        "\n",
        "            # Extract the generated answer\n",
        "            answer = AgentChatResponse.response\n",
        "\n",
        "            # Extract retrieved document segments (accessing node and then its content)\n",
        "            retrieved_segments = [node.node.text for node in AgentChatResponse.source_nodes]\n",
        "\n",
        "            # Return both the answer and the retrieved document segments\n",
        "            return answer, retrieved_segments\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"KeyError: {e}\")\n",
        "            return \"An error occurred while processing your query.\", []\n",
        "\n",
        "    @property\n",
        "    def _prompt(self):\n",
        "        return \"\"\"\n",
        "            You are a professional AI assistant that answers questions based on the provided document.\n",
        "            Use relevant information from the document to answer any questions accurately and concisely.\n",
        "\n",
        "            if answer is not available in document, try to answer on the base of previous conversation,if u know the answer properly.\n",
        "\n",
        "            If the information is not available in document and in previous conversation, simply state that the answer is not in the document.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzSGtq7H9Wp-"
      },
      "source": [
        "### **Generative Response Creation**\n",
        "\n",
        "Once the relevant document chunks are retrieved, the system moves to the response generation phase:\n",
        "\n",
        "1. **Input to Generative Model**:\n",
        "   - The retrieved chunks are concatenated to form the context. This context is passed as input to the **Gemini** or **Google Generative AI** model.\n",
        "   - The model uses this context to generate a response that addresses the user’s query.\n",
        "\n",
        "2. **Contextual Answer Generation**:\n",
        "   - The generative model leverages the information in the context while applying its internal knowledge to generate a fluent, coherent, and contextually relevant answer.\n",
        "   - Depending on the complexity of the query, the response may combine information from multiple chunks.\n",
        "\n",
        "3. **Final Output**:\n",
        "   - The generated response is returned to the user, providing an answer that is both factual (based on retrieved content) and linguistically natural (thanks to the generative capabilities of the language model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "5Sw3X4_a9Gwg",
        "outputId": "800a1335-453f-4bf0-b22f-00f493bc4bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents loaded successfully from PDF file\n",
            "Deleted existing collection: collection\n",
            "Created new collection: collection\n",
            "Knowledgebase created successfully!\n",
            "Debug: Sending query to chat engine\n",
            "Debug: Received response from chat engine\n",
            "Chat Bot\n",
            "The document is a selection of verses from the Bhagavad Gita, a Hindu scripture, focusing on the concepts of selfless service, enlightenment, and achieving union with the Supreme Being. \n",
            "\n",
            "RAG\n",
            "['(3.05) The deluded \\nones, who restrain their organs of action but mentally dwell upon \\nthe sense enjoyment, are called hypocrites. (3.06)  \\nWhy one should serve others?  \\nOne who controls the senses by a trained and purified mind \\nand intellect, and engages the organs of action to selfless service, \\nis superior, O Arjuna. (3.07) Perform your obligatory duty because \\nworking is indeed better than sitting idle. Even the maintena nce of \\nyour body would be impossible without work. (3.08) Human beings  \\nare bound by work that is not performed as a selfless service. \\nTherefore, O Arjuna, becoming free from attachment  to the fruits \\nof work, do your duty efficiently as a service to Me. (3 .09)                      Bhagavad -Gita                             9 \\n \\n The first commandment of the creator  \\nBrahm a, the creator, in the beginning created human beings \\ntogether with selfless service (Sev a, sacrifice) and said: By Seva \\nor sacrifice of self -interest  you shall prosper, and Seva shall fulfill \\nall your desire s. (3.10) Help the celestial controllers (Devas)  with self-\\nless service and they will help you. Thus helping each other, you shall \\nattain the Supreme goal of human life. (3.11) The celestial control-\\nlers, pleased  by selfless service, will give you desired objects  even \\nwithout asking for it . One who enjoys the gift of celestial s without \\noffering them anything in return is, indeed, a thief. (3.12) The right-\\neous who eat the remnants of selfless service are freed from  all \\nsins, but the impious who cook food only for themselves (without \\nfirst offering to Me, or sharing with others) , in truth, eat sin (3.13) . \\nThe physical body of living beings is made up of and sustained by \\nfood; food is created by God from five basic el ements. God is \\npleased by the sacrifice  (Qurbani)  of self-interest : The s elfless  phil-\\nanthropic work , and b lesses selfless persons . Selfless work or sac-\\nrifice  is our duty. Duty is prescribed in the scriptures .', '(5.16) Persons whose mind and intellect are totally merged \\nwith the Source , who are firmly devoted to the Source , who have \\nthe Source  as their supreme goal and sole refuge, and whose im-\\npurities are destroyed by the knowledge of the Source , do not take \\nbirth again. (5.17)  \\nAdditional marks of an enlightened person  \\nAn enlightened person — by perceiving the Lord in all  — looks \\nat a learned and humble priest, an outcast, even a cow, an ele-\\nphant, or a dog with an equal eye. (5.18) Everything has been ac-\\ncomplished in this very life by the one whose mind is set in equalit y. \\nSuch a person has realized God, because God is flawless and im-\\npartial. (5.19) One who neither rejoices on obtaining what is pleas-\\nant, nor grieves on obtaining the unpleasant, who has a steady \\nmind, who is undeluded, and who is a knower of the Supreme  Be-\\ning, such a person eternally abides with the Supreme  Being. (5.20) \\nSuch a person who is in union with the Supreme  Being becomes \\nunattached to sensual pleasures by discovering the joy of the Self \\nthrough contemplation, and enjoys spiritual  bliss. (5.21) Sensual \\npleasures are, in truth, the source of misery, and have a beginning \\nand an end. Therefore , the wise do not rejoice sensual pleasures. \\n(5.22) One who is able to withstand the impulse of lust and anger \\nbefore death is a yogi, and a happy person. (5.23) One who finds \\nhappiness with the Eternal Being, who rejoices Eternal Being \\nwithin, and who is illuminated by Self -knowledge; such a yogi at-\\ntains Nirv ana, and goes to Eternal  Being. (5.24) Seers whose sins \\n(or imperfections) are destroyed, whose doubts have been dis-\\npelled by Self -knowledge, whose minds are disciplined, and who \\nare engaged in the welfare of all beings, attain the Supreme Being. \\n(5.25) They , who are free from lu st and anger, who have subdued \\nthe mind and senses, and who have known the Self, easily attain \\nNirvana.']\n"
          ]
        }
      ],
      "source": [
        "document_path = \"bhagavad-gita-in-english-source-file.pdf\"\n",
        "chatbot = QAChatbot(document_path)\n",
        "response = chatbot.interact_with_llm(\"What is the main purpose of the document?\")\n",
        "print(\"Chat Bot\")\n",
        "print(response[0])\n",
        "print(\"RAG\")\n",
        "print(response[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "ywSWner-9sBi",
        "outputId": "7f85305a-987c-473e-da59-1eeea0cc2372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug: Sending query to chat engine\n",
            "Debug: Received response from chat engine\n",
            "Chat Bot\n",
            "Here are 10 life lessons from the provided excerpt of the Bhagavad Gita, focusing on the themes of enlightenment, self-realization, and spiritual liberation:\n",
            "\n",
            "1. **True Liberation:**  The path to liberation lies in merging with the Source, dedicating oneself to it, and letting go of impurities through knowledge. This leads to freedom from the cycle of rebirth. (5.16)\n",
            "2. **Equality and Compassion:**  An enlightened person sees everyone and everything with equal regard, recognizing the divine within all beings, regardless of their social status or perceived worth. (5.17)\n",
            "3. **Inner Peace through Equanimity:**  Achieving inner peace and realizing God comes from cultivating a mind that remains undisturbed by external circumstances, both pleasant and unpleasant. (5.18, 5.19)\n",
            "4. **The Joy of Self-Realization:**  True happiness comes from connecting with the Self through contemplation and experiencing the joy of spiritual bliss, rather than seeking fleeting pleasures. (5.20)\n",
            "5. **The Impermanence of Sensual Pleasures:**  Sensual pleasures are temporary and ultimately lead to suffering. The wise understand this and seek lasting joy through spiritual connection. (5.21)\n",
            "6. **Conquering Desire and Anger:**  Controlling lust and anger is crucial for spiritual progress. Those who can overcome these impulses before death are considered yogis and attain happiness. (5.22)\n",
            "7. **Finding Happiness in the Eternal:**  True happiness lies in finding joy in the Eternal Being, within oneself, and being illuminated by self-knowledge. This leads to Nirvana and union with the Divine. (5.23)\n",
            "8. **Destroying Sins and Doubts:**  Self-knowledge helps to eliminate sins and doubts, leading to a disciplined mind and a focus on the welfare of all beings. This path leads to the Supreme Being. (5.24)\n",
            "9. **Subduing the Mind and Senses:**  Freedom from lust and anger, along with a subdued mind and senses, are essential for achieving Nirvana and attaining liberation. (5.25)\n",
            "10. **The Path of Self-Discipline and Compassion:**  The path to enlightenment involves self-discipline, conquering inner negativity, and cultivating compassion for all beings. This leads to a state of liberation and union with the Divine. \n",
            "\n",
            "These lessons emphasize the importance of inner transformation, self-awareness, and selfless service as the key to achieving true happiness and liberation. \n",
            "\n",
            "RAG\n",
            "['(Twelfth  print in China,  Oct 2015 -June 2020 ) \\nTo read more about the picture above, Click:  \\nhttps://www.gita -society.com/pdf/china.pdf  \\nOUR PUBLICATIONS:  \\nPaperbacks  ||  eBooks  || HOME', '(5.16) Persons whose mind and intellect are totally merged \\nwith the Source , who are firmly devoted to the Source , who have \\nthe Source  as their supreme goal and sole refuge, and whose im-\\npurities are destroyed by the knowledge of the Source , do not take \\nbirth again. (5.17)  \\nAdditional marks of an enlightened person  \\nAn enlightened person — by perceiving the Lord in all  — looks \\nat a learned and humble priest, an outcast, even a cow, an ele-\\nphant, or a dog with an equal eye. (5.18) Everything has been ac-\\ncomplished in this very life by the one whose mind is set in equalit y. \\nSuch a person has realized God, because God is flawless and im-\\npartial. (5.19) One who neither rejoices on obtaining what is pleas-\\nant, nor grieves on obtaining the unpleasant, who has a steady \\nmind, who is undeluded, and who is a knower of the Supreme  Be-\\ning, such a person eternally abides with the Supreme  Being. (5.20) \\nSuch a person who is in union with the Supreme  Being becomes \\nunattached to sensual pleasures by discovering the joy of the Self \\nthrough contemplation, and enjoys spiritual  bliss. (5.21) Sensual \\npleasures are, in truth, the source of misery, and have a beginning \\nand an end. Therefore , the wise do not rejoice sensual pleasures. \\n(5.22) One who is able to withstand the impulse of lust and anger \\nbefore death is a yogi, and a happy person. (5.23) One who finds \\nhappiness with the Eternal Being, who rejoices Eternal Being \\nwithin, and who is illuminated by Self -knowledge; such a yogi at-\\ntains Nirv ana, and goes to Eternal  Being. (5.24) Seers whose sins \\n(or imperfections) are destroyed, whose doubts have been dis-\\npelled by Self -knowledge, whose minds are disciplined, and who \\nare engaged in the welfare of all beings, attain the Supreme Being. \\n(5.25) They , who are free from lu st and anger, who have subdued \\nthe mind and senses, and who have known the Self, easily attain \\nNirvana.']\n"
          ]
        }
      ],
      "source": [
        "response = chatbot.interact_with_llm(\"give top 10 life lesson from this pdf\")\n",
        "print(\"Chat Bot\")\n",
        "print(response[0])\n",
        "print(\"RAG\")\n",
        "print(response[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "2_vGgxFd9zmj",
        "outputId": "917e1021-1fb7-4d33-d930-c3c76cde9fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug: Sending query to chat engine\n",
            "Debug: Received response from chat engine\n",
            "Chat Bot\n",
            "This document does not contain information about the weather. \n",
            "\n",
            "RAG\n",
            "['I am the thunderbolt among weap-\\nons, and I am Cupid for procreation. (10.27 -28) I am the water god \\nand the manes. I am the controller of death. I am death among the \\nhealers, lion among the beasts, and the king of birds among birds. \\n(10.29-30) I am the wind among the purifiers and Lord R ama \\namong the warriors. I am the crocodile among the fishes and the \\nholy Gang a river among the rivers. (10.31)  \\nI am the beginning, the middle, and the end of all creation, O \\nArjuna. Among knowledge I am  knowledge of the supreme Self. I \\nam logic of the logician. (10.32) I am the letter ‘A’ among the al-\\nphabets. I am the dual compound among the compound words. I \\nam the endless time. I am the sustainer of all, and have faces on \\nall sides (or I am omniscient) . (10.33) I am the all -devouring death \\nand also the origin of future beings. I am the seven goddesses or \\nguardian angels presiding over the seven qualities : Fame, pros-\\nperity, speech, memory, intellect, resolve, and forgiveness. (10.34) \\nI am the Vedic hymns . I am G ayatri mantra among the Vedic man-\\ntras. I am November -December among the months, I am the spring \\namong the seasons. (10.35) I am gambling of the cheats, splendor \\nof the splendid, victory of the victorious, resolution of the resolute, \\nand goodness of  the good. (10.36)  \\nI am Krishna, Arjuna , sage Vyasa among the sages, and \\nUshan a among the great poets. (10.37) I am the power of rulers, \\nthe statesmanship of the seekers of victory, I am silence among \\nthe secrets, and the Self -knowledge of the knowledgeable. (10.38) \\nI am the origin of all beings, O Arjuna. There is nothing, animate or \\ninanimate, that  can exist without Me.', 'They have neither purity nor good conduct nor truthfulness. \\n(16.07) They say that the world is unreal, without a substratum, \\nwithou t God, and without an order. The world is caused by sexual \\nunion of man and woman alone and nothing else. (16.08) Adhering \\nto this wrong, atheistic view, these degraded souls — with small \\nintellect and cruel deeds — are born as enemies for the destruction \\nof the world. (16.09) Filled with insatiable desires, hypocrisy, pride, \\nand arrogance; holding wrong views due to delusion; they act with \\nimpure motives, (16.10) obsessed with endless anxiety lasting until \\ndeath, considering sense gratification their highe st aim, and con-\\nvinced that sense pleasure is everything; (16.11)  \\nBound by hundreds of ties of desire and enslaved by lust and \\nanger, they strive to obtain wealth by unlawful means to fulfill sen-\\nsual pleasures. They think: (16.12) This has been gained by m e \\ntoday; I shall fulfill this desire; I have this much wealth and will have \\nmore wealth in the future; (16.13) that enemy has been slain by \\nme, and I shall slay others also. I am the lord. I am the enjoyer. I \\nam successful, powerful, and happy; (16.14)  \\nI am rich and born in a noble family. Who is equal to me? I shall \\nperform sacrifice, I shall give charity, and I shall rejoice. (16.15) \\nThus deluded by ignorance, bewildered by many fancies, entan-\\ngled in the net of delusion, addicted to the enjoyment of sens ual \\npleasures, they fall into a foul hell. (16.16) Self -conceited, stub-\\nborn, filled with pride and intoxication of wealth, they perform sac-\\nrifice , charity, etc. only in name for show, and not according to 40                           International Gita Society  \\n \\n scriptural injunction. (16.17) These malicious peop le cling to ego-\\nism, power, arrogance, lust, anger , greed, hatred ; and they deny \\nMy presence in their own body and in others’ bodies.']\n"
          ]
        }
      ],
      "source": [
        "response = chatbot.interact_with_llm(\"what is the weather today\")\n",
        "print(\"Chat Bot\")\n",
        "print(response[0])\n",
        "print(\"RAG\")\n",
        "print(response[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "veiCGbvi_LrT",
        "outputId": "fc770055-2534-466d-8e40-f533f2be9a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug: Sending query to chat engine\n",
            "Debug: Received response from chat engine\n",
            "Chat Bot\n",
            "As an AI, I don't have memory of past conversations or personal information about you.  If you'd like to tell me who you are, I'd be happy to get to know you! 😊 \n",
            "\n",
            "RAG\n",
            "['I am the thunderbolt among weap-\\nons, and I am Cupid for procreation. (10.27 -28) I am the water god \\nand the manes. I am the controller of death. I am death among the \\nhealers, lion among the beasts, and the king of birds among birds. \\n(10.29-30) I am the wind among the purifiers and Lord R ama \\namong the warriors. I am the crocodile among the fishes and the \\nholy Gang a river among the rivers. (10.31)  \\nI am the beginning, the middle, and the end of all creation, O \\nArjuna. Among knowledge I am  knowledge of the supreme Self. I \\nam logic of the logician. (10.32) I am the letter ‘A’ among the al-\\nphabets. I am the dual compound among the compound words. I \\nam the endless time. I am the sustainer of all, and have faces on \\nall sides (or I am omniscient) . (10.33) I am the all -devouring death \\nand also the origin of future beings. I am the seven goddesses or \\nguardian angels presiding over the seven qualities : Fame, pros-\\nperity, speech, memory, intellect, resolve, and forgiveness. (10.34) \\nI am the Vedic hymns . I am G ayatri mantra among the Vedic man-\\ntras. I am November -December among the months, I am the spring \\namong the seasons. (10.35) I am gambling of the cheats, splendor \\nof the splendid, victory of the victorious, resolution of the resolute, \\nand goodness of  the good. (10.36)  \\nI am Krishna, Arjuna , sage Vyasa among the sages, and \\nUshan a among the great poets. (10.37) I am the power of rulers, \\nthe statesmanship of the seekers of victory, I am silence among \\nthe secrets, and the Self -knowledge of the knowledgeable. (10.38) \\nI am the origin of all beings, O Arjuna. There is nothing, animate or \\ninanimate, that  can exist without Me.', '(5.16) Persons whose mind and intellect are totally merged \\nwith the Source , who are firmly devoted to the Source , who have \\nthe Source  as their supreme goal and sole refuge, and whose im-\\npurities are destroyed by the knowledge of the Source , do not take \\nbirth again. (5.17)  \\nAdditional marks of an enlightened person  \\nAn enlightened person — by perceiving the Lord in all  — looks \\nat a learned and humble priest, an outcast, even a cow, an ele-\\nphant, or a dog with an equal eye. (5.18) Everything has been ac-\\ncomplished in this very life by the one whose mind is set in equalit y. \\nSuch a person has realized God, because God is flawless and im-\\npartial. (5.19) One who neither rejoices on obtaining what is pleas-\\nant, nor grieves on obtaining the unpleasant, who has a steady \\nmind, who is undeluded, and who is a knower of the Supreme  Be-\\ning, such a person eternally abides with the Supreme  Being. (5.20) \\nSuch a person who is in union with the Supreme  Being becomes \\nunattached to sensual pleasures by discovering the joy of the Self \\nthrough contemplation, and enjoys spiritual  bliss. (5.21) Sensual \\npleasures are, in truth, the source of misery, and have a beginning \\nand an end. Therefore , the wise do not rejoice sensual pleasures. \\n(5.22) One who is able to withstand the impulse of lust and anger \\nbefore death is a yogi, and a happy person. (5.23) One who finds \\nhappiness with the Eternal Being, who rejoices Eternal Being \\nwithin, and who is illuminated by Self -knowledge; such a yogi at-\\ntains Nirv ana, and goes to Eternal  Being. (5.24) Seers whose sins \\n(or imperfections) are destroyed, whose doubts have been dis-\\npelled by Self -knowledge, whose minds are disciplined, and who \\nare engaged in the welfare of all beings, attain the Supreme Being. \\n(5.25) They , who are free from lu st and anger, who have subdued \\nthe mind and senses, and who have known the Self, easily attain \\nNirvana.']\n"
          ]
        }
      ],
      "source": [
        "response = chatbot.interact_with_llm(\"Do u know who am i?\")\n",
        "print(\"Chat Bot\")\n",
        "print(response[0])\n",
        "print(\"RAG\")\n",
        "print(response[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtMPJO7g8rD2"
      },
      "source": [
        "### **Flow Overview**\n",
        "\n",
        "1. **User Query**: User submits a query.\n",
        "2. **Query Embedding**: The query is converted into an embedding using **GeminiEmbedding**.\n",
        "3. **Vector Search**: The query embedding is compared to the stored document embeddings in **ChromaVectorStore**.\n",
        "4. **Top-k Retrieval**: The most similar document chunks are retrieved.\n",
        "5. **Response Generation**: The retrieved chunks are passed to a generative model, which synthesizes a response.\n",
        "6. **Answer Delivery**: The system delivers the generated answer to the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsjvkNzzxLki"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
